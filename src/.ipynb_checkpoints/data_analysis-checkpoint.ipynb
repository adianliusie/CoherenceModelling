{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "class DataHandler():\n",
    "    def __init__(self, data_src):\n",
    "        path = self.get_path(data_src)\n",
    "        data = self.load_data(path)\n",
    "        self.data = [i['sents'] for i in data]\n",
    "    \n",
    "    def get_path(self, data_src):\n",
    "        base_dir = '/home/alta/Conversational/OET/al826/2021'\n",
    "        path_dict = {'wiki':f'{base_dir}/data/unlabeled/wiki_100000.json',\n",
    "                    'WSJ':f'{base_dir}/data/coherence/WSJ_train.json'}\n",
    "        return path_dict[data_src]\n",
    "    \n",
    "    def load_data(self, path):\n",
    "        with open(path) as jsonFile:\n",
    "            return json.load(jsonFile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils.py\n",
    "from transformers import BertTokenizerFast, RobertaTokenizerFast\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class UtilClass:\n",
    "    def __init__(self, system, lim=300_000):\n",
    "        if system in ['bert', 'electra']:\n",
    "            self.tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "            self.CLS, self.SEP = [101], [102]\n",
    "            self.embeddings = None\n",
    "            \n",
    "        elif system == 'roberta':\n",
    "            self.tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "            self.CLS, self.SEP = [0], [2]\n",
    "            self.embeddings = None\n",
    "\n",
    "        elif system in ['glove', 'word2vec']:\n",
    "            path = self.get_embedding_path(system)\n",
    "            tok_dict, embed_matrix = self.read_embeddings(path, lim)\n",
    "            self.tokenizer = FakeTokenizer(tok_dict)\n",
    "            self.embeddings = torch.Tensor(embed_matrix)\n",
    "            self.CLS, self.SEP = [], []\n",
    "\n",
    "        else:\n",
    "            raise ValueError('invalid system')\n",
    "            \n",
    "    def get_embedding_path(self, name):\n",
    "        base_dir = '/home/alta/Conversational/OET/al826/2021'\n",
    "        if name == 'glove': path = f'{base_dir}/data/embeddings/glove.840B.300d.txt'\n",
    "        elif name == 'word2vec': path = f'{base_dir}/data/embeddings/word2vec.txt'\n",
    "        else: raise ValueError('invalid word embedding system') \n",
    "        return path \n",
    "\n",
    "    def read_embeddings(self, path, limit=300_000):\n",
    "        with open(path, 'r') as file:\n",
    "            _ = next(file)\n",
    "            tok_dict = {'[UNK]':0}\n",
    "            embed_matrix = []\n",
    "            for index, line in tqdm(zip(range(limit), file), total=limit):\n",
    "                word, *embedding = line.split()\n",
    "                if len(embedding) == 300 and word not in tok_dict:\n",
    "                    embed_matrix.append([float(i) for i in embedding])\n",
    "                    tok_dict[word] = len(tok_dict)\n",
    "        return tok_dict, embed_matrix\n",
    "\n",
    "#Making the tokenizer the same format as huggingface to better interface with code\n",
    "class FakeTokenizer:\n",
    "    def __init__(self, tok_dict):\n",
    "        self.tok_dict = tok_dict\n",
    "        self.reverse_dict = {v:k for k,v in self.tok_dict.items()}\n",
    "\n",
    "    def tokenize_word(self, w):\n",
    "        if w in self.tok_dict:  output = self.tok_dict[w]\n",
    "        else: output = 0\n",
    "        return output\n",
    "\n",
    "    def tokenize(self, x):\n",
    "        tokenized_words = [self.tokenize_word(i) for i in x.split()]\n",
    "        x = type('TokenizedInput', (), {})()\n",
    "        setattr(x, 'input_ids', tokenized_words)\n",
    "        return x\n",
    "\n",
    "    def decode(self, x):\n",
    "        return ' '.join([self.reverse_dict[i] for i in x])\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.tokenize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corrupter.py\n",
    "import time\n",
    "import random\n",
    "\n",
    "def create_corrupted_set(coherent, num_fake, schemes=[1], args=[1]):\n",
    "    corrupted_set = []\n",
    "    fail = 0\n",
    "    while len(corrupted_set) < num_fake and fail<50:\n",
    "        _, incoherent = create_incoherent(coherent, schemes, args)\n",
    "        if (incoherent not in corrupted_set) and (incoherent != coherent): corrupted_set.append(incoherent)\n",
    "        else: fail += 1\n",
    "    return corrupted_set\n",
    "\n",
    "def create_incoherent(coherent, schemes, args):\n",
    "    r = random.choice(schemes)\n",
    "    if   r == 1: incoherent = random_shuffle(coherent)\n",
    "    elif r == 2: incoherent = random_swaps(coherent, args[0])\n",
    "    elif r == 3: incoherent = random_neighbour_swaps(coherent, args[0])\n",
    "    elif r == 4: incoherent = random_deletion(coherent, args[0])\n",
    "    elif r == 5: incoherent = local_word_swaps(coherent, *args)\n",
    "    else: raise Exception\n",
    "    return coherent, incoherent\n",
    "\n",
    "def random_shuffle(conversation):\n",
    "    incoherent = conversation.copy()\n",
    "    random.shuffle(incoherent)\n",
    "    return incoherent\n",
    "\n",
    "def random_swaps(conversation, num_swaps=1):\n",
    "    incoherent = conversation.copy()\n",
    "    indices = random.sample(range(0, len(incoherent)), min(2*num_swaps, 2*int(len(incoherent)/2)))\n",
    "\n",
    "    for i in range(0, len(indices), 2):\n",
    "        ind_1, ind_2 = indices[i], indices[i+1]\n",
    "        incoherent[ind_1], incoherent[ind_2] = incoherent[ind_2], incoherent[ind_1]\n",
    "    return incoherent\n",
    "\n",
    "def random_neighbour_swaps(conversation, num_swaps=1):\n",
    "    incoherent = conversation.copy()\n",
    "    indices = random.sample(range(1, len(incoherent)), num_swaps)\n",
    "    for i in indices:\n",
    "        incoherent[i], incoherent[i-1] = incoherent[i-1], incoherent[i]\n",
    "    return incoherent\n",
    "\n",
    "def random_deletion(conversation, num_delete=1):\n",
    "    incoherent = conversation.copy()\n",
    "    _ = [conversation.pop(-1) for i in range(num_delete)]\n",
    "    indices = random.sample(range(1, len(incoherent)-1), num_delete)\n",
    "    indices.sort(reverse=True)\n",
    "    for i in indices:\n",
    "        incoherent.pop(i)\n",
    "    return incoherent\n",
    "\n",
    "def local_word_swaps(conversation, num_sents=1, num_word_swaps=1):\n",
    "    incoherent = conversation.copy()\n",
    "    indices = random.sample(range(0, len(incoherent)), num_sents)\n",
    "    for i in indices:\n",
    "        words = incoherent[i].split()\n",
    "        positions = random.sample(range(0, len(words)), min(2*num_word_swaps, 2*(len(words)//2)))\n",
    "        for j in range(0, len(positions), 2):\n",
    "            ind_1, ind_2 = positions[j], positions[j+1]\n",
    "            words[ind_1], words[ind_2] = words[ind_2], words[ind_1]\n",
    "        sentence = ' '.join(words)\n",
    "        incoherent[i] = sentence\n",
    "    return incoherent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batcher.py\n",
    "from collections import namedtuple\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "\n",
    "flatten = lambda doc: [word for sent in doc for word in sent]\n",
    "Batch = namedtuple('Batch', ['ids', 'mask'])\n",
    "\n",
    "class Batcher:\n",
    "    def __init__(self, bsz=8, schemes=[1], args=None, max_len=512, U=None):\n",
    "        self.bsz = bsz\n",
    "        self.schemes = schemes\n",
    "        self.args = args\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        self.tokenizer = U.tokenizer\n",
    "        self.CLS = U.CLS\n",
    "        self.SEP = U.SEP\n",
    "        \n",
    "        self.device = torch.device('cpu')\n",
    "        \n",
    "    def make_batches(self, documents, c_num, hier=False):\n",
    "        coherent = documents.copy()\n",
    "        random.shuffle(coherent)\n",
    "        coherent = [self.tokenize_doc(doc) for doc in tqdm(coherent)]\n",
    "        \n",
    "        if not hier: coherent = [doc for doc in coherent if len(self.flatten_doc(doc)) < self.max_len] \n",
    "        else:  coherent = [doc for doc in coherent if max([len(i) for i in doc]) < self.max_len] \n",
    "                    \n",
    "        cor_pairs = self.corupt_pairs(coherent, c_num)\n",
    "        batches = [cor_pairs[i:i+self.bsz] for i in range(0,len(cor_pairs), self.bsz)]\n",
    "        batches = [self.prep_batch(batch, hier) for batch in batches]\n",
    "        return batches        \n",
    "    \n",
    "    def corupt_pairs(self, coherent, c_num):\n",
    "        incoherent = [create_corrupted_set(doc, c_num, self.schemes, self.args) for doc in coherent]\n",
    "        examples = []\n",
    "        for pos, neg_set in zip(coherent, incoherent):\n",
    "            for neg in neg_set:\n",
    "                examples.append([pos, neg])\n",
    "        return examples\n",
    "\n",
    "    def prep_batch(self, pairs, hier=False):\n",
    "        if hier == False:\n",
    "            coherent, incoherent = zip(*pairs)\n",
    "            coherent = [self.flatten_doc(doc) for doc in coherent]\n",
    "            incoherent = [self.flatten_doc(doc) for doc in incoherent]\n",
    "\n",
    "            pos_batch = self.batchify(coherent)\n",
    "            neg_batch = self.batchify(incoherent)\n",
    "            return pos_batch, neg_batch\n",
    "        \n",
    "        else:\n",
    "            return [[self.batchify(coh), self.batchify(inc)] for coh, inc in pairs]\n",
    "        \n",
    "    def tokenize_doc(self, document):\n",
    "        return [self.tokenizer(sent).input_ids for sent in document]\n",
    "    \n",
    "    def flatten_doc(self, document):\n",
    "        ids = self.CLS + flatten([sent[1:-1] for sent in document]) + self.SEP\n",
    "        return ids\n",
    "    \n",
    "    def batchify(self, batch):\n",
    "        max_len = max([len(i) for i in batch])\n",
    "        ids = [doc + [0]*(max_len-len(doc)) for doc in batch]\n",
    "        mask = [[1]*len(doc) + [0]*(max_len-len(doc)) for doc in batch]\n",
    "        ids = torch.LongTensor(ids).to(self.device)\n",
    "        mask = torch.FloatTensor(mask).to(self.device)\n",
    "        return Batch(ids, mask)\n",
    "\n",
    "    def to(self, device):\n",
    "        self.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertConfig, BertModel, RobertaModel, ElectraModel\n",
    "\n",
    "class DocumentClassifier(nn.Module):\n",
    "    def __init__(self, class_num=1, system=None, hier=None, embeds=None):  \n",
    "        super().__init__()\n",
    "        self.classifier = nn.Linear(300, class_num)\n",
    "        \n",
    "        if system in ['bert','roberta','electra']: self.sent_encoder = TransEncoder(system) \n",
    "        elif system in ['glove', 'word2vec']:      self.sent_encoder = BilstmEncoder(embeds) \n",
    "\n",
    "        self.hier = hier\n",
    "        if hier == 'transformer': self.doc_encoder = HierTransEncoder(hier)\n",
    "        elif hier == 'bilstm': self.doc_encoder = HierTransEncoder(hier)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        y = self.sent_encoder(x, mask)\n",
    "        if self.hier:\n",
    "            y = y.unsqueeze(0)\n",
    "            y = self.doc_encoder(y)\n",
    "        y = self.classifier(y)\n",
    "        return y\n",
    "\n",
    "class TransEncoder(nn.Module):\n",
    "    def __init__(self, name):\n",
    "        super().__init__()\n",
    "        self.transformer = self.get_transformer(name)\n",
    "        self.linear = nn.Linear(768, 300)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        h1 = self.transformer(input_ids=x, attention_mask=mask).last_hidden_state[:,0]\n",
    "        h1 = self.linear(h1)\n",
    "        return h1 \n",
    "\n",
    "    def get_transformer(self, name):\n",
    "        if name == 'bert':      transformer = BertModel.from_pretrained('bert-base-uncased', return_dict=True)\n",
    "        elif name == 'roberta': transformer = RobertaModel.from_pretrained('roberta-base', return_dict=True)\n",
    "        elif name == 'electra': transformer = ElectraModel.from_pretrained('google/electra-base-discriminator')\n",
    "        elif name == 'hier': transformer = BertModel()\n",
    "        else: raise Exception\n",
    "        return transformer\n",
    "\n",
    "class BilstmEncoder(nn.Module):\n",
    "    def __init__(self, embeddings):\n",
    "        super().__init__()\n",
    "        self.bilstm = nn.LSTM(input_size=300, hidden_size=150, num_layers=2, \n",
    "                                    bias=True, batch_first=True, dropout=0, bidirectional=True)\n",
    "        self.embeddings = nn.Embedding.from_pretrained(embeddings)\n",
    "        \n",
    "    def forward(self, x, mask):  \n",
    "        x = self.embeddings(x)\n",
    "        mask_lens = torch.sum(mask, dim=-1).cpu()\n",
    "        x_padded = torch.nn.utils.rnn.pack_padded_sequence(x, mask_lens, batch_first=True, enforce_sorted=False)\n",
    "        output, _ = self.bilstm(x_padded) \n",
    "        h1, unpacked_len = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "        h1 = torch.mean(h1, dim=1)\n",
    "        return h1\n",
    "\n",
    "class HierTransEncoder(nn.Module):\n",
    "    def __init__(self, name, hsz=300):\n",
    "        super().__init__()\n",
    "        config = BertConfig(hidden_size=hsz, num_hidden_layers=12, num_attention_heads=12, intermediate_size=4*hsz)\n",
    "        self.transformer = BertModel(config)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        hidden_vectors = self.transformer(inputs_embeds=x, return_dict=True).last_hidden_state[:,0]\n",
    "        return hidden_vectors \n",
    "\n",
    "class HierBilstmEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bilstm = nn.LSTM(input_size=300, hidden_size=150, num_layers=1, bias=True, \n",
    "                              batch_first=True, dropout=0, bidirectional=True)\n",
    "        \n",
    "    def forward(self, x):  \n",
    "        h1, _ = self.bilstm(x) \n",
    "        h1 = torch.mean(h1, dim=1)\n",
    "        return output\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nD = DataHandler('wiki')\\nU = UtilClass('bert', 100_000)\\nmodel = DocumentClassifier(1, 'bert', embeds=U.embeddings)\\nB = Batcher(4, [1], [1], 200, U)\\n\\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\\nmodel.to(device)\\nB.to(device)\\n\\nfor k, batch in enumerate(B.make_batches(D.data[:1000], c_num=1, hier='transformer')):\\n    for pos, neg in batch:\\n        print(pos.ids.shape)\\n        y = model(pos.ids, pos.mask)\\n        print(y.shape)\\n    time.sleep(5)\\n    print('--'*20)\\n\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "D = DataHandler('wiki')\n",
    "U = UtilClass('bert', 100_000)\n",
    "model = DocumentClassifier(1, 'bert', embeds=U.embeddings)\n",
    "B = Batcher(4, [1], [1], 200, U)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "B.to(device)\n",
    "\n",
    "for k, batch in enumerate(B.make_batches(D.data[:1000], c_num=1, hier='transformer')):\n",
    "    for pos, neg in batch:\n",
    "        print(pos.ids.shape)\n",
    "        y = model(pos.ids, pos.mask)\n",
    "        print(y.shape)\n",
    "    time.sleep(5)\n",
    "    print('--'*20)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel = DocumentClassifier(1, 'bert', 'transformer')\\n\\n#TEMP\\nD = DataHandler('wiki')\\nU = UtilClass('bert', 100_000)\\nB = Batcher(4, [1], [1], 200, U)\\n\\nfor k, batch in enumerate(B.make_batches(D.data[:1000], c_num=1, hier='bert')):\\n    for pos, neg in batch:\\n        y = model(pos.ids, pos.mask)\\n        print(y.shape)\\n    time.sleep(5)\\n    print('--'*20)\\n\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TEMP\n",
    "\"\"\"\n",
    "model = DocumentClassifier(1, 'bert', 'transformer')\n",
    "\n",
    "#TEMP\n",
    "D = DataHandler('wiki')\n",
    "U = UtilClass('bert', 100_000)\n",
    "B = Batcher(4, [1], [1], 200, U)\n",
    "\n",
    "for k, batch in enumerate(B.make_batches(D.data[:1000], c_num=1, hier='bert')):\n",
    "    for pos, neg in batch:\n",
    "        y = model(pos.ids, pos.mask)\n",
    "        print(y.shape)\n",
    "    time.sleep(5)\n",
    "    print('--'*20)\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "class log_sigmoid_loss(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.log_sigmoid = nn.LogSigmoid()\n",
    "\n",
    "        def forward(self, inputs):\n",
    "            log_likelihood = self.log_sigmoid(inputs)\n",
    "            loss =  -1 * torch.mean(log_likelihood)\n",
    "            return loss\n",
    "\n",
    "class ExperimentHandler:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.cross_loss = nn.CrossEntropyLoss()\n",
    "        self.log_sigmoid_loss = log_sigmoid_loss()\n",
    "\n",
    "    def train(self, config):\n",
    "        D = DataHandler(config.data_src)\n",
    "        U = UtilClass(config.system, config.embed_lim)\n",
    "        B = Batcher(config.bsz, config.schemes, config.args, config.max_len, U)\n",
    "        \n",
    "        model = DocumentClassifier(1, config.system, config.hier, embeds=U.embeddings)\n",
    "        self.model = model\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "        if config.scheduling:\n",
    "              SGD_steps = (len(train_data)*config.epochs)/self.bsz\n",
    "              lambda1 = lambda i: 10*i/SGD_steps if i <= SGD_steps/10 else 1 - ((i - 0.1*SGD_steps)/(0.9*SGD_steps))\n",
    "              scheduler = LambdaLR(optimizer, lr_lambda=lambda1)\n",
    "        \n",
    "        model.to(self.device)\n",
    "        B.to(self.device) \n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            analysis_loss, acc = 0, np.zeros(2)\n",
    "            \n",
    "            for k, batch in enumerate(B.make_batches(D.data[:10000], config.c_num, config.hier)):\n",
    "                if config.hier in ['transformer', 'bilstm']:\n",
    "                    loss = 0\n",
    "                    for pos, neg in batch:\n",
    "                        y_pos = model(pos.ids, pos.mask)\n",
    "                        y_neg = model(neg.ids, neg.mask)\n",
    "                        loss += self.log_sigmoid_loss(y_pos - y_neg)/len(batch)\n",
    "                        acc += [(y_pos>y_neg).item(), 1]\n",
    "                else:\n",
    "                    pos, neg = batch\n",
    "                    y_pos = model(pos.ids, pos.mask)\n",
    "                    y_neg = model(neg.ids, neg.mask)\n",
    "                    loss = self.log_sigmoid_loss(y_pos - y_neg)\n",
    "                    acc += [sum(y_pos - y_neg > 0).item(), len(y_pos)]\n",
    "            \n",
    "                analysis_loss += loss.item()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if config.scheduling: scheduler.step()\n",
    "\n",
    "                if k%config.debug_sz==0 and k!=0:\n",
    "                    print(f'{k:<5} {analysis_loss/config.debug_sz:.3f}   {acc[0]/acc[1]:.3f}')\n",
    "                    analysis_loss, acc = 0, np.zeros(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9411b83880c4493daf3b942b33d7afc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100   0.497   0.744\n",
      "200   0.400   0.818\n",
      "300   0.453   0.782\n",
      "400   0.383   0.813\n",
      "500   0.395   0.783\n",
      "600   0.380   0.823\n",
      "700   0.359   0.813\n",
      "800   0.327   0.848\n",
      "900   0.391   0.810\n",
      "1000  0.376   0.813\n",
      "1100  0.359   0.810\n",
      "1200  0.355   0.827\n",
      "1300  0.365   0.813\n",
      "1400  0.369   0.810\n",
      "1500  0.378   0.812\n",
      "1600  0.385   0.810\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1803b4ae88f47adabd825dd7d4c05a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100   0.259   0.866\n",
      "200   0.292   0.852\n",
      "300   0.292   0.845\n",
      "400   0.299   0.862\n",
      "500   0.295   0.823\n",
      "600   0.254   0.870\n",
      "700   0.260   0.865\n",
      "800   0.300   0.847\n",
      "900   0.317   0.843\n",
      "1000  0.291   0.832\n",
      "1100  0.279   0.863\n",
      "1200  0.269   0.852\n",
      "1300  0.309   0.855\n",
      "1400  0.286   0.857\n",
      "1500  0.307   0.847\n",
      "1600  0.287   0.848\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6afbbf802de44131859fb572cad5d338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100   0.246   0.893\n",
      "200   0.203   0.895\n",
      "300   0.226   0.880\n",
      "400   0.217   0.882\n",
      "500   0.245   0.872\n",
      "600   0.239   0.895\n",
      "700   0.233   0.870\n",
      "800   0.241   0.883\n",
      "900   0.251   0.877\n",
      "1000  0.214   0.882\n",
      "1100  0.228   0.882\n",
      "1200  0.208   0.885\n",
      "1300  0.233   0.867\n",
      "1400  0.253   0.887\n",
      "1500  0.180   0.905\n",
      "1600  0.250   0.867\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d1f52034ad44ff8f42345b4bda61c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100   0.181   0.896\n",
      "200   0.177   0.905\n"
     ]
    }
   ],
   "source": [
    "config_dict = {'bsz':6, 'lr':1e-5, 'epochs':5, 'scheduling':False, \n",
    "               'system':'bert', 'embed_lim':100000, 'hier':'bilstm', \n",
    "               'data_src':'wiki', 'c_num':1, 'schemes':[1], 'args':None, 'max_len':512,\n",
    "               'debug_sz':100}\n",
    "\n",
    "ConfigTruple = namedtuple('Config', config_dict)\n",
    "config = ConfigTruple(**config_dict)\n",
    "\n",
    "E = ExperimentHandler()\n",
    "E.train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DocumentClassifier(c_num=1, system='glove', hier=False, embeds=U.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = torch.ones([8,32], dtype=torch.long)\n",
    "\n",
    "print(model(ids, ids).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = UtilClass('glove')\n",
    "D = DataHandler('wiki')\n",
    "B = Batcher(U)\n",
    "model = DocumentClassifier(1, system='bert', hier=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
