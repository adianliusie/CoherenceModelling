{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import torch\n",
    "import random\n",
    "import json\n",
    "import copy\n",
    "import time\n",
    "\n",
    "class DataHandler():\n",
    "    def __init__(self, data_src):\n",
    "        path = self.get_path(data_src)\n",
    "        data = self.load_data(path)\n",
    "        self.data = [i['sents'] for i in data]\n",
    "    \n",
    "    def get_path(self, data_src):\n",
    "        base_dir = '/home/alta/Conversational/OET/al826/2021'\n",
    "        path_dict = {'wiki':f'{base_dir}/data/unlabeled/wiki_100000.json',\n",
    "                    'WSJ':f'{base_dir}/data/coherence/WSJ_train.json'}\n",
    "        return path_dict[data_src]\n",
    "    \n",
    "    def load_data(self, path):\n",
    "        with open(path) as jsonFile:\n",
    "            return json.load(jsonFile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils.py\n",
    "from transformers import BertTokenizerFast, RobertaTokenizerFast\n",
    "\n",
    "class UtilClass:\n",
    "    def __init__(self, system, lim=300_000):\n",
    "        if system in ['bert', 'electra']:\n",
    "            self.tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "            self.CLS, self.SEP = [101], [102]\n",
    "            self.embeddings = None\n",
    "            \n",
    "        elif system == 'roberta':\n",
    "            self.tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "            self.CLS, self.SEP = [0], [2]\n",
    "            self.embeddings = None\n",
    "\n",
    "        elif system in ['glove', 'word2vec']:\n",
    "            path = self.get_embedding_path(system)\n",
    "            tok_dict, embed_matrix = self.read_embeddings(path, lim)\n",
    "            self.tokenizer = FakeTokenizer(tok_dict)\n",
    "            self.embeddings = torch.Tensor(embed_matrix)\n",
    "            self.CLS, self.SEP = [], []\n",
    "        \n",
    "        else:\n",
    "            raise ValueError('invalid system')\n",
    "            \n",
    "    def get_embedding_path(self, name):\n",
    "        base_dir = '/home/alta/Conversational/OET/al826/2021'\n",
    "        if name == 'glove': path = f'{base_dir}/data/embeddings/glove.840B.300d.txt'\n",
    "        elif name == 'word2vec': path = f'{base_dir}/data/embeddings/word2vec.txt'\n",
    "        else: raise ValueError('invalid word embedding system') \n",
    "        return path \n",
    "\n",
    "    def read_embeddings(self, path, limit=300_000):\n",
    "        with open(path, 'r') as file:\n",
    "            _ = next(file)\n",
    "            tok_dict = {'[UNK]':0}\n",
    "            embed_matrix = []\n",
    "            for index, line in tqdm(zip(range(limit), file), total=limit):\n",
    "                word, *embedding = line.split()\n",
    "                if len(embedding) == 300 and word not in tok_dict:\n",
    "                    embed_matrix.append([float(i) for i in embedding])\n",
    "                    tok_dict[word] = len(tok_dict)\n",
    "        return tok_dict, embed_matrix\n",
    "\n",
    "#Making the tokenizer the same format as huggingface to better interface with code\n",
    "class FakeTokenizer:\n",
    "    def __init__(self, tok_dict):\n",
    "        self.tok_dict = tok_dict\n",
    "        self.reverse_dict = {v:k for k,v in self.tok_dict.items()}\n",
    "\n",
    "    def tokenize_word(self, w):\n",
    "        if w in self.tok_dict:  output = self.tok_dict[w]\n",
    "        else: output = 0\n",
    "        return output\n",
    "\n",
    "    def tokenize(self, x):\n",
    "        tokenized_words = [self.tokenize_word(i) for i in x.split()]\n",
    "        x = type('TokenizedInput', (), {})()\n",
    "        setattr(x, 'input_ids', tokenized_words)\n",
    "        return x\n",
    "\n",
    "    def decode(self, x):\n",
    "        return ' '.join([self.reverse_dict[i] for i in x])\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.tokenize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corrupter.py\n",
    "import time\n",
    "\n",
    "def create_corrupted_set(coherent, num_fake, schemes=[1], args=[1]):\n",
    "    corrupted_set = []\n",
    "    fail = 0\n",
    "    while len(corrupted_set) < num_fake and fail<50:\n",
    "        _, incoherent = create_incoherent(coherent, schemes, args)\n",
    "        if (incoherent not in corrupted_set) and (incoherent != coherent): corrupted_set.append(incoherent)\n",
    "        else: fail += 1\n",
    "    return corrupted_set\n",
    "\n",
    "def create_incoherent(coherent, schemes, args):\n",
    "    r = random.choice(schemes)\n",
    "    if   r == 1: incoherent = random_shuffle(coherent)\n",
    "    elif r == 2: incoherent = random_swaps(coherent, args[0])\n",
    "    elif r == 3: incoherent = random_neighbour_swaps(coherent, args[0])\n",
    "    elif r == 4: incoherent = random_deletion(coherent, args[0])\n",
    "    elif r == 5: incoherent = local_word_swaps(coherent, *args)\n",
    "    else: raise Exception\n",
    "    return coherent, incoherent\n",
    "\n",
    "def random_shuffle(conversation):\n",
    "    incoherent = conversation.copy()\n",
    "    random.shuffle(incoherent)\n",
    "    return incoherent\n",
    "\n",
    "def random_swaps(conversation, num_swaps=1):\n",
    "    incoherent = conversation.copy()\n",
    "    indices = random.sample(range(0, len(incoherent)), min(2*num_swaps, 2*int(len(incoherent)/2)))\n",
    "\n",
    "    for i in range(0, len(indices), 2):\n",
    "        ind_1, ind_2 = indices[i], indices[i+1]\n",
    "        incoherent[ind_1], incoherent[ind_2] = incoherent[ind_2], incoherent[ind_1]\n",
    "    return incoherent\n",
    "\n",
    "def random_neighbour_swaps(conversation, num_swaps=1):\n",
    "    incoherent = conversation.copy()\n",
    "    indices = random.sample(range(1, len(incoherent)), num_swaps)\n",
    "    for i in indices:\n",
    "        incoherent[i], incoherent[i-1] = incoherent[i-1], incoherent[i]\n",
    "    return incoherent\n",
    "\n",
    "def random_deletion(conversation, num_delete=1):\n",
    "    incoherent = conversation.copy()\n",
    "    _ = [conversation.pop(-1) for i in range(num_delete)]\n",
    "    indices = random.sample(range(1, len(incoherent)-1), num_delete)\n",
    "    indices.sort(reverse=True)\n",
    "    for i in indices:\n",
    "        incoherent.pop(i)\n",
    "    return incoherent\n",
    "\n",
    "def local_word_swaps(conversation, num_sents=1, num_word_swaps=1):\n",
    "    incoherent = conversation.copy()\n",
    "    indices = random.sample(range(0, len(incoherent)), num_sents)\n",
    "    for i in indices:\n",
    "        words = incoherent[i].split()\n",
    "        positions = random.sample(range(0, len(words)), min(2*num_word_swaps, 2*(len(words)//2)))\n",
    "        for j in range(0, len(positions), 2):\n",
    "            ind_1, ind_2 = positions[j], positions[j+1]\n",
    "            words[ind_1], words[ind_2] = words[ind_2], words[ind_1]\n",
    "        sentence = ' '.join(words)\n",
    "        incoherent[i] = sentence\n",
    "    return incoherent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batcher.py\n",
    "from collections import namedtuple\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "flatten = lambda doc: [word for sent in doc for word in sent]\n",
    "Batch = namedtuple('Batch', ['ids', 'mask'])\n",
    "\n",
    "class Batcher:\n",
    "    def __init__(self, U):\n",
    "        self.bsz = 8\n",
    "        self.schemes = [2]\n",
    "        self.args = [1]\n",
    "        \n",
    "        self.tokenizer = U.tokenizer\n",
    "        self.CLS = U.CLS\n",
    "        self.SEP = U.SEP\n",
    "        \n",
    "    def make_batches(self, documents, c_num, hier=False):\n",
    "        coherent = documents.copy()\n",
    "        random.shuffle(coherent)\n",
    "        coherent = [self.tokenize_doc(doc) for doc in tqdm(coherent)]\n",
    "        cor_pairs = self.corupt_pairs(coherent, c_num)\n",
    "        batches = [cor_pairs[i:i+self.bsz] for i in range(0,len(cor_pairs), self.bsz)]\n",
    "        batches = [self.prep_batch(batch, hier) for batch in batches]\n",
    "        return batches        \n",
    "    \n",
    "    def corupt_pairs(self, coherent, c_num):\n",
    "        incoherent = [create_corrupted_set(doc, c_num, self.schemes, self.args) for doc in coherent]\n",
    "        examples = []\n",
    "        for pos, neg_set in zip(coherent, incoherent):\n",
    "            for neg in neg_set:\n",
    "                examples.append([pos, neg])\n",
    "        return examples\n",
    "\n",
    "    def prep_batch(self, pairs, hier=False):\n",
    "        if hier == False:\n",
    "            coherent, incoherent = zip(*pairs)\n",
    "            coherent = [self.flatten_doc(doc) for doc in coherent]\n",
    "            incoherent = [self.flatten_doc(doc) for doc in incoherent]\n",
    "\n",
    "            pos_batch = self.batchify(coherent)\n",
    "            neg_batch = self.batchify(incoherent)\n",
    "            return pos_batch, neg_batch\n",
    "        \n",
    "        elif hier == True:\n",
    "            return [[self.batchify(coh), self.batchift(inc)] for coh, inc in pairs]\n",
    "\n",
    "    def tokenize_doc(self, document):\n",
    "        return [self.tokenizer(sent).input_ids for sent in document]\n",
    "    \n",
    "    def flatten_doc(self, document):\n",
    "        ids = self.CLS + flatten([sent[1:-1] for sent in document]) + self.SEP\n",
    "        return ids\n",
    "    \n",
    "    def batchify(self, batch):\n",
    "        max_len = max([len(i) for i in batch])\n",
    "        ids = [doc + [0]*(max_len-len(doc)) for doc in batch]\n",
    "        mask = [[1]*len(doc) + [0]*(max_len-len(doc)) for doc in batch]\n",
    "        ids = torch.LongTensor(ids) #.to(self.device)\n",
    "        mask = torch.FloatTensor(mask) #.to(self.device)\n",
    "        return Batch(ids, mask)\n",
    "    \n",
    "\n",
    "U = UtilClass('bert')\n",
    "D = DataHandler('wiki')\n",
    "B = Batcher(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertConfig, BertModel, RobertaModel, ElectraModel\n",
    "\n",
    "class DocumentClassifier(nn.Module):\n",
    "    def __init__(self, class_number=1, system=None, hier=None, embeds=None):  \n",
    "        super().__init__()\n",
    "        \n",
    "        if system in ['bert','roberta','electra']: self.sent_encoder = TransEncoder(system) \n",
    "        elif system in ['glove', 'word2vec']:      self.sent_encoder = BilstmEncoder(embeds) \n",
    "\n",
    "        if hier in ['bert','roberta','electra']: self.doc_encoder = TransEncoder(hier)\n",
    "        elif hier in ['glove', 'wiki']: self.doc_encoder = BilstmEncoder(hier)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        y = self.sent_encoder(x, mask)\n",
    "        if self.hier:\n",
    "            y = self.doc_encoder(y)\n",
    "        y = self.classifier(y)\n",
    "        return y\n",
    "\n",
    "class TransEncoder(nn.Module):\n",
    "    def __init__(self, name):\n",
    "        super().__init__()\n",
    "        self.transformer = self.get_transformer(name)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        hidden_vectors = self.transformer(input_ids=x, attention_mask=mask).last_hidden_state[:,0]\n",
    "        return hidden_vectors \n",
    "\n",
    "    def get_transformer(self, name):\n",
    "        if name == 'bert':      transformer = BertModel.from_pretrained('bert-base-uncased', return_dict=True)\n",
    "        elif name == 'roberta': transformer = RobertaModel.from_pretrained('roberta-base', return_dict=True)\n",
    "        elif name == 'electra': transformer = ElectraModel.from_pretrained('google/electra-base-discriminator')\n",
    "        elif name == 'hier': transformer = BertModel()\n",
    "        else: raise Exception\n",
    "        return transformer\n",
    "\n",
    "class BilstmEncoder(nn.Module):\n",
    "    def __init__(self, embeddings):\n",
    "        super().__init__()\n",
    "        self.bilstm = nn.LSTM(input_size=300, hidden_size=150, num_layers=1, \n",
    "                                    bias=True, batch_first=True, dropout=0, bidirectional=True)\n",
    "        self.embeddings = nn.Embedding.from_pretrained(embeddings)\n",
    "        \n",
    "    def forward(self, x, mask):  \n",
    "        embeds = self.embeddings(x)\n",
    "        mask_lens = torch.sum(mask, dim=-1)\n",
    "        x_padded = torch.nn.utils.rnn.pack_padded_sequence(embeds, mask_lens, batch_first=True, enforce_sorted=False)\n",
    "        output, _ = self.bilstm(x_padded) \n",
    "        h1, unpacked_len = torch.nn.utils.rnn.pad_packed_sequence(output)\n",
    "        return h1\n",
    "\n",
    "class HierTransEncoder(nn.Module):\n",
    "    def __init__(self, name, hsz=300):\n",
    "        super().__init__()\n",
    "        config = BertConfig(hidden_size=hsz, num_hidden_layers=12, num_attention_heads=12, intermediate_size=4*hsz)\n",
    "        self.transformer = BertModel(config)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        hidden_vectors = self.transformer(inputs_embeds=x).last_hidden_state[:,0]\n",
    "        return hidden_vectors \n",
    "\n",
    "class HierBilstmEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bilstm = nn.LSTM(input_size=300, hidden_size=150, num_layers=1, bias=True, \n",
    "                              batch_first=True, dropout=0, bidirectional=True)\n",
    "        \n",
    "    def forward(self, x):  \n",
    "        output, _ = self.bilstm(x) \n",
    "        return output\n",
    "\n",
    "\"\"\"\n",
    "U = UtilClass('glove', 5000)\n",
    "model = DocumentClassifier(1, system='glove', hier=False, embeds=U.embeddings)\n",
    "\n",
    "D = DataHandler()\n",
    "d = D.pair_corupt('wiki_50000', [1], [1])\n",
    "\n",
    "for batch in U.prep_batches(d, 2):\n",
    "    y = [model(i.ids, i.mask) for i in batch]\n",
    "\"\"\"\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "a = [[1,4,7],[2,5,8],[3,6,9]]\n",
    "\n",
    "flatten = lambda doc: [word for sent in doc for word in sent]\n",
    "\n",
    "print(flatten(zip(*a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
